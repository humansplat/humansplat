import os
from glob import glob

import cv2
import numpy as np
import torch
from skimage.io import imread
from skimage.transform import estimate_transform, warp
from torch.utils.data import Dataset


def video2sequence(video_path):
    videofolder = video_path.split(".")[0]
    util.check_mkdir(videofolder)
    video_name = video_path.split("/")[-1].split(".")[0]
    vidcap = cv2.VideoCapture(video_path)
    success, image = vidcap.read()
    count = 0
    imagepath_list = []
    while success:
        imagepath = "{}/{}_frame{:04d}.jpg".format(videofolder, video_name, count)
        cv2.imwrite(imagepath, image)  # save frame as JPEG file
        success, image = vidcap.read()
        count += 1
        imagepath_list.append(imagepath)
    print("video frames are stored in {}".format(videofolder))
    return imagepath_list


class TestData(Dataset):
    def __init__(
        self, testpath, iscrop=True, crop_size=224, scale=2.4, detector="hand"
    ):
        """
        testpath: folder, imagepath_list, image path, video path
        """
        if isinstance(testpath, list):
            self.imagepath_list = testpath
        elif os.path.isdir(testpath):
            self.imagepath_list = (
                glob(testpath + "/*.jpg")
                + glob(testpath + "/*.png")
                + glob(testpath + "/*.bmp")
            )
        elif os.path.isfile(testpath) and (testpath[-3:] in ["jpg", "png", "bmp"]):
            self.imagepath_list = [testpath]
        elif os.path.isfile(testpath) and (
            testpath[-3:] in ["mp4", "csv", "vid", "ebm"]
        ):
            self.imagepath_list = video2sequence(testpath)
        else:
            print(f"please check the test path: {testpath}")
            exit()
        print("total {} images".format(len(self.imagepath_list)))
        self.imagepath_list = sorted(self.imagepath_list)
        self.crop_size = crop_size
        self.scale = scale
        self.iscrop = iscrop
        self.resolution_inp = crop_size

        # TODO: add hand detector
        self.detector = None
        print(
            f"No hand detector is available, please make sure the input hand images are well-cropped"
        )

    def __len__(self):
        return len(self.imagepath_list)

    def bbox2point(self, left, right, top, bottom, type="bbox"):
        """bbox from detector and landmarks are different"""
        if type == "kpt68":
            old_size = (right - left + bottom - top) / 2 * 1.1
            center = np.array(
                [right - (right - left) / 2.0, bottom - (bottom - top) / 2.0]
            )
        elif type == "bbox":
            old_size = (right - left + bottom - top) / 2
            center = np.array(
                [
                    right - (right - left) / 2.0,
                    bottom - (bottom - top) / 2.0 + old_size * 0.12,
                ]
            )
        else:
            raise NotImplementedError
        return old_size, center

    def __getitem__(self, index):
        imagepath = self.imagepath_list[index]
        imagename = imagepath.split("/")[-1].split(".")[0]

        image = np.array(imread(imagepath))
        if len(image.shape) == 2:
            image = image[:, :, None].repeat(1, 1, 3)
        if len(image.shape) == 3 and image.shape[2] > 3:
            image = image[:, :, :3]

        h, w, _ = image.shape
        if self.iscrop and self.detector is not None:
            bbox = self.face_detector.run(image)
            if len(bbox) < 4:
                print("no face detected! run original image")
                left = 0
                right = h - 1
                top = 0
                bottom = w - 1
            else:
                left = bbox[0]
                right = bbox[2]
                top = bbox[1]
                bottom = bbox[3]
            old_size, center = self.bbox2point(left, right, top, bottom, type="kpt68")
            size = int(old_size * self.scale)
            src_pts = np.array(
                [
                    [center[0] - size / 2, center[1] - size / 2],
                    [center[0] - size / 2, center[1] + size / 2],
                    [center[0] + size / 2, center[1] - size / 2],
                ]
            )
        else:
            src_pts = np.array([[0, 0], [0, h - 1], [w - 1, 0]])

        DST_PTS = np.array(
            [[0, 0], [0, self.resolution_inp - 1], [self.resolution_inp - 1, 0]]
        )
        tform = estimate_transform("similarity", src_pts, DST_PTS)

        image = image / 255.0

        dst_image = warp(
            image,
            tform.inverse,
            output_shape=(self.resolution_inp, self.resolution_inp),
        )
        dst_image = dst_image.transpose(2, 0, 1)
        return {
            "image": torch.tensor(dst_image).float(),
            "name": imagename,
            "imagepath": imagepath
            # 'tform': tform,
            # 'original_image': torch.tensor(image.transpose(2,0,1)).float(),
        }
